{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import gluon, init\n",
    "from mxnet.gluon import loss as gloss, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation of `softmax`, we first subtract $\\max(z_i)$ from all $z_i$. This won't change the result of the calculation. But after the substraction there maybe some `z_j` that is very negative. Thus, $e^{z_j}$ will be very close to zero and might be rounded to zero. So we combine the `softmax` and `cross_entropy` together to avoid calculating $e^{z_j}$ directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7882, train acc 0.748, test acc 0.802\n",
      "epoch 2, loss 0.5745, train acc 0.810, test acc 0.823\n",
      "epoch 3, loss 0.5296, train acc 0.824, test acc 0.830\n",
      "epoch 4, loss 0.5059, train acc 0.829, test acc 0.832\n",
      "epoch 5, loss 0.4889, train acc 0.835, test acc 0.840\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try adjusting the hyper-parameters, such as batch size, epoch, and learning rate, to see what the results are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.8060, train acc 0.743, test acc 0.812\n",
      "epoch 2, loss 0.5712, train acc 0.806, test acc 0.812\n",
      "epoch 3, loss 0.5275, train acc 0.821, test acc 0.841\n",
      "epoch 4, loss 0.5063, train acc 0.827, test acc 0.844\n",
      "epoch 5, loss 0.4858, train acc 0.834, test acc 0.847\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "net.initialize(init.Normal(sigma=0.01), force_reinit=True)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.8051, train acc 0.742, test acc 0.812\n",
      "epoch 2, loss 0.5803, train acc 0.805, test acc 0.833\n",
      "epoch 3, loss 0.5227, train acc 0.821, test acc 0.835\n",
      "epoch 4, loss 0.4983, train acc 0.830, test acc 0.845\n",
      "epoch 5, loss 0.4872, train acc 0.833, test acc 0.837\n",
      "epoch 6, loss 0.4741, train acc 0.837, test acc 0.845\n",
      "epoch 7, loss 0.4655, train acc 0.840, test acc 0.826\n",
      "epoch 8, loss 0.4720, train acc 0.838, test acc 0.852\n",
      "epoch 9, loss 0.4585, train acc 0.842, test acc 0.852\n",
      "epoch 10, loss 0.4492, train acc 0.844, test acc 0.843\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "net.initialize(init.Normal(sigma=0.01), force_reinit=True)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.1196, train acc 0.706, test acc 0.825\n",
      "epoch 2, loss 1.1644, train acc 0.783, test acc 0.831\n",
      "epoch 3, loss 1.0939, train acc 0.792, test acc 0.830\n",
      "epoch 4, loss 1.0196, train acc 0.799, test acc 0.841\n",
      "epoch 5, loss 1.0268, train acc 0.804, test acc 0.851\n",
      "epoch 6, loss 0.9546, train acc 0.808, test acc 0.838\n",
      "epoch 7, loss 0.9468, train acc 0.810, test acc 0.848\n",
      "epoch 8, loss 0.9631, train acc 0.809, test acc 0.851\n",
      "epoch 9, loss 0.9427, train acc 0.812, test acc 0.836\n",
      "epoch 10, loss 0.8656, train acc 0.818, test acc 0.855\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "trainer.set_learning_rate(0.3)\n",
    "\n",
    "net.initialize(init.Normal(sigma=0.01), force_reinit=True)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why might the test accuracy decrease again after a while? How could we fix this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
