{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, we assume that prediction can be expressed as a *linear combination* of the input features. Given a collection of data points $\\mathbf{X}$, we try to find the weight vector $\\mathbf{w}$ and bias term $b$ that approximately associate data points $x_i$ with their coresponding labels $y_i$.\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathbf{W}\\mathbf{x} + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we choose a non-negative number as a error. The smaller the value, the smaller the error. In linear regression, the square function is a commom choice.\n",
    "\n",
    "$$L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^n\\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Normal Distribution and Squared Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we assume the Gaussian noise is added to the data, we can write out the *likelihood* fuction that describe the probability of seeing a particular $y$ for given $\\mathbf{x}$:\n",
    "\n",
    "$$p(y|\\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right)$$\n",
    "\n",
    "A good way of finding the most likely values of $b$ and $\\\\mathbf{w}$ is to maximize the *likelihood* of the entire dataset, or equally, minimize the *Negative Log-Likelihood*:\n",
    "\n",
    "$$-\\log P(Y|X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2$$\n",
    "\n",
    "The second term is identical to the objective of the square loss function of the linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Assume that we have some data $x_1, \\ldots x_n \\in \\mathbb{R}$. Our goal is to find a constant $b$ such that $\\sum_i (x_i - b)^2$ is minimized.\n",
    "    * Find the optimal closed form solution.\n",
    "    * What does this mean in terms of the Normal distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Assume that we want to solve the optimization problem for linear regression with quadratic loss explicitly in closed form. To keep things simple, you can omit the bias $b$ from the problem.\n",
    "    * Rewrite the problem in matrix and vector notation (hint - treat all the data as a single matrix).\n",
    "    * Compute the gradient of the optimization problem with respect to $w$.\n",
    "    * Find the closed form solution by solving a matrix equation.\n",
    "    * When might this be better than using stochastic gradient descent (i.e. the incremental optimization approach that we discussed above)? When will this break (hint - what happens for high-dimensional $x$, what if many observations are very similar)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Assume that the noise model governing the additive noise $\\epsilon$ is the exponential distribution. That is, $p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$.\n",
    "    * Write out the negative log-likelihood of the data under the model $-\\log p(Y|X)$.\n",
    "    * Can you find a closed form solution?\n",
    "    * Suggest a stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint - what happens near the stationary point as we keep on updating the parameters). Can you fix this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compare the runtime of the two methods of adding two vectors using other packages (such as NumPy) or other programming languages (such as MATLAB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(1000000)\n",
    "b = np.arange(1000000)\n",
    "c = np.zeros_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 431 ms, sys: 4.2 ms, total: 435 ms\n",
      "Wall time: 443 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(a)):\n",
    "    c[i] = a[i] + b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.92 ms, sys: 1.43 ms, total: 4.35 ms\n",
      "Wall time: 2.56 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "c = a + b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
