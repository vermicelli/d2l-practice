{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import gluon, init\n",
    "from mxnet.gluon import loss as gloss, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7982, train acc 0.696, test acc 0.832\n",
      "epoch 2, loss 0.4873, train acc 0.821, test acc 0.849\n",
      "epoch 3, loss 0.4318, train acc 0.840, test acc 0.860\n",
      "epoch 4, loss 0.3976, train acc 0.854, test acc 0.858\n",
      "epoch 5, loss 0.3695, train acc 0.863, test acc 0.867\n",
      "epoch 6, loss 0.3532, train acc 0.869, test acc 0.875\n",
      "epoch 7, loss 0.3374, train acc 0.877, test acc 0.871\n",
      "epoch 8, loss 0.3274, train acc 0.880, test acc 0.878\n",
      "epoch 9, loss 0.3174, train acc 0.883, test acc 0.877\n",
      "epoch 10, loss 0.3044, train acc 0.887, test acc 0.882\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})\n",
    "num_epochs = 10\n",
    "\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try adding a few more hidden layers to see how the result changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.2036, train acc 0.135, test acc 0.205\n",
      "epoch 2, loss 1.7057, train acc 0.322, test acc 0.508\n",
      "epoch 3, loss 1.0973, train acc 0.586, test acc 0.445\n",
      "epoch 4, loss 0.8512, train acc 0.665, test acc 0.743\n",
      "epoch 5, loss 0.5751, train acc 0.770, test acc 0.778\n",
      "epoch 6, loss 0.5137, train acc 0.807, test acc 0.769\n",
      "epoch 7, loss 0.4665, train acc 0.825, test acc 0.839\n",
      "epoch 8, loss 0.4441, train acc 0.834, test acc 0.854\n",
      "epoch 9, loss 0.4177, train acc 0.843, test acc 0.857\n",
      "epoch 10, loss 0.3946, train acc 0.852, test acc 0.851\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(128, activation='relu'))\n",
    "net.add(nn.Dense(64, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01), force_reinit=True)\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})\n",
    "\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Try out different activation functions. Which ones work best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7386, train acc 0.733, test acc 0.824\n",
      "epoch 2, loss 0.5115, train acc 0.812, test acc 0.838\n",
      "epoch 3, loss 0.4651, train acc 0.830, test acc 0.851\n",
      "epoch 4, loss 0.4319, train acc 0.842, test acc 0.860\n",
      "epoch 5, loss 0.4183, train acc 0.848, test acc 0.860\n",
      "epoch 6, loss 0.3980, train acc 0.854, test acc 0.868\n",
      "epoch 7, loss 0.3834, train acc 0.860, test acc 0.869\n",
      "epoch 8, loss 0.3780, train acc 0.862, test acc 0.864\n",
      "epoch 9, loss 0.3638, train acc 0.867, test acc 0.864\n",
      "epoch 10, loss 0.3608, train acc 0.867, test acc 0.870\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='softsign'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01), force_reinit=True)\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})\n",
    "\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try out different initializations of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bilinear',\n",
       " 'Constant',\n",
       " 'FusedRNN',\n",
       " 'InitDesc',\n",
       " 'Initializer',\n",
       " 'LSTMBias',\n",
       " 'Load',\n",
       " 'MSRAPrelu',\n",
       " 'Mixed',\n",
       " 'NDArray',\n",
       " 'Normal',\n",
       " 'One',\n",
       " 'Orthogonal',\n",
       " 'Uniform',\n",
       " 'Xavier',\n",
       " 'Zero']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[init_method for init_method in dir(init) if init_method[0].isupper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7367, train acc 0.728, test acc 0.829\n",
      "epoch 2, loss 0.4693, train acc 0.827, test acc 0.854\n",
      "epoch 3, loss 0.4137, train acc 0.848, test acc 0.865\n",
      "epoch 4, loss 0.3826, train acc 0.859, test acc 0.867\n",
      "epoch 5, loss 0.3574, train acc 0.869, test acc 0.872\n",
      "epoch 6, loss 0.3443, train acc 0.873, test acc 0.868\n",
      "epoch 7, loss 0.3266, train acc 0.879, test acc 0.882\n",
      "epoch 8, loss 0.3143, train acc 0.884, test acc 0.882\n",
      "epoch 9, loss 0.3044, train acc 0.887, test acc 0.884\n",
      "epoch 10, loss 0.2984, train acc 0.889, test acc 0.883\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Uniform(), force_reinit=True)\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})\n",
    "\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
